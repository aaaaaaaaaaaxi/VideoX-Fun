#!/usr/bin/env python
"""
æ‰¹é‡ Wan2.2 Animate LoRA æ¨ç†è„šæœ¬
=====================================

ç”¨äºæ‰¹é‡ç”Ÿæˆå¤šä¸ªè§†é¢‘ï¼Œæ”¯æŒä»é…ç½®æ–‡ä»¶è¯»å–å‚æ•°ã€‚
"""

import argparse
import json
import os
import logging
from pathlib import Path
from typing import List, Dict, Any
from tqdm import tqdm

from inference_animate_lora_advanced import Wan2_2AnimateAdvancedInference, GenerationConfig


logger = logging.getLogger(__name__)


def load_config(config_path: str) -> List[Dict[str, Any]]:
    """ä»JSONæ–‡ä»¶åŠ è½½é…ç½®åˆ—è¡¨ã€‚"""
    with open(config_path, 'r', encoding='utf-8') as f:
        config_data = json.load(f)

    if isinstance(config_data, dict):
        if 'prompts' in config_data:
            # æ ¼å¼1: {"prompts": [...], "common": {...}}
            common = config_data.get('common', {})
            prompts = config_data['prompts']
            # åˆå¹¶é€šç”¨é…ç½®
            for prompt_config in prompts:
                prompt_config.update(common)
            return prompts
        else:
            # æ ¼å¼2: å•ä¸ªé…ç½®
            return [config_data]
    elif isinstance(config_data, list):
        # æ ¼å¼3: é…ç½®åˆ—è¡¨
        return config_data
    else:
        raise ValueError("é…ç½®æ–‡ä»¶æ ¼å¼é”™è¯¯")


def save_config_to_json(configs: List[Dict[str, Any]], output_path: str):
    """å°†é…ç½®ä¿å­˜ä¸ºJSONæ–‡ä»¶ã€‚"""
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(configs, f, indent=2, ensure_ascii=False)


def create_sample_config(output_path: str = "sample_config.json"):
    """åˆ›å»ºç¤ºä¾‹é…ç½®æ–‡ä»¶ã€‚"""
    sample_configs = [
        {
            "name": "nature_video",
            "prompt": "A beautiful waterfall in a lush green forest, sunlight filtering through trees",
            "negative_prompt": "blurry, low quality, bad art, distorted",
            "num_frames": 49,
            "height": 480,
            "width": 720,
            "guidance_scale": 7.5,
            "seed": 42
        },
        {
            "name": "cityscape",
            "prompt": "Futuristic cityscape at night, neon lights, flying cars",
            "negative_prompt": "poor quality, blurry, cartoonish",
            "num_frames": 81,
            "height": 512,
            "width": 512,
            "guidance_scale": 8.0,
            "seed": 123
        },
        {
            "name": "animal_animation",
            "prompt": "A cute cat playing with a ball of yarn, smooth animation",
            "negative_prompt": "static, blurry, deformed",
            "num_frames": 25,
            "height": 384,
            "width": 640,
            "guidance_scale": 6.5,
            "seed": 456
        }
    ]

    save_config_to_json(sample_configs, output_path)
    print(f"ç¤ºä¾‹é…ç½®å·²ä¿å­˜åˆ°: {output_path}")


def batch_inference(
    base_model_path: str,
    lora_path: str,
    config_file: str,
    output_dir: str = "./batch_outputs",
    max_workers: int = 1,
    device: str = "cuda",
    dtype: str = "float16"
):
    """æ‰¹é‡æ¨ç†ã€‚"""
    # åŠ è½½é…ç½®
    configs = load_config(config_file)

    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(output_dir, exist_ok=True)

    # åˆ›å»ºæˆåŠŸå’Œå¤±è´¥è®°å½•
    successful_generations = []
    failed_generations = []

    logger.info(f"å¼€å§‹æ‰¹é‡ç”Ÿæˆ {len(configs)} ä¸ªè§†é¢‘...")

    # ä¸ºæ¯ä¸ªé…ç½®ç”Ÿæˆè§†é¢‘
    for config in tqdm(configs, desc="ç”Ÿæˆè¿›åº¦"):
        try:
            # è·³è¿‡æ²¡æœ‰promptçš„é…ç½®
            if 'prompt' not in config:
                logger.warning(f"é…ç½® {config.get('name', 'unknown')} ç¼ºå°‘promptï¼Œè·³è¿‡")
                failed_generations.append({
                    'config': config,
                    'error': "Missing prompt in config"
                })
                continue

            # åˆ›å»ºç”Ÿæˆé…ç½®
            gen_config = GenerationConfig(
                base_model_path=base_model_path,
                lora_path=lora_path,
                output_dir=output_dir,
                prompt=config['prompt'],
                negative_prompt=config.get('negative_prompt', ''),
                height=config.get('height', 480),
                width=config.get('width', 720),
                num_frames=config.get('num_frames', 49),
                fps=config.get('fps', 8),
                num_inference_steps=config.get('num_inference_steps', 50),
                guidance_scale=config.get('guidance_scale', 7.0),
                eta=config.get('eta', 0.0),
                num_videos_per_prompt=config.get('num_videos_per_prompt', 1),
                device=device,
                dtype=dtype,
                scheduler=config.get('scheduler', 'euler'),
                seed=config.get('seed'),
                mode=config.get('mode', 'text_to_video'),
                control_type=config.get('control_type'),
                control_image=config.get('control_image'),
                start_image=config.get('start_image'),
                reference_image=config.get('reference_image')
            )

            # ç”Ÿæˆè§†é¢‘
            generator = Wan2_2AnimateAdvancedInference(gen_config)
            output_path = generator.generate()

            # è®°å½•æˆåŠŸ
            result = {
                'name': config.get('name', 'unnamed'),
                'prompt': config['prompt'],
                'output_path': output_path,
                'config': {k: v for k, v in config.items() if k != 'prompt'}
            }
            successful_generations.append(result)

            logger.info(f"âœ… {config.get('name', 'unknown')} ç”ŸæˆæˆåŠŸ: {output_path}")

        except Exception as e:
            logger.error(f"âŒ {config.get('name', 'unknown')} ç”Ÿæˆå¤±è´¥: {str(e)}")
            failed_generations.append({
                'name': config.get('name', 'unknown'),
                'prompt': config.get('prompt', ''),
                'error': str(e),
                'config': config
            })

    # ä¿å­˜ç»“æœ
    results = {
        'successful': successful_generations,
        'failed': failed_generations,
        'summary': {
            'total': len(configs),
            'successful': len(successful_generations),
            'failed': len(failed_generations),
            'success_rate': len(successful_generations) / len(configs) if configs else 0
        }
    }

    results_path = os.path.join(output_dir, 'batch_results.json')
    save_config_to_json([results], results_path)

    # æ‰“å°æ‘˜è¦
    print(f"\nğŸ“Š æ‰¹é‡ç”Ÿæˆæ‘˜è¦:")
    print(f"   æ€»æ•°: {results['summary']['total']}")
    print(f"   æˆåŠŸ: {results['summary']['successful']}")
    print(f"   å¤±è´¥: {results['summary']['failed']}")
    print(f"   æˆåŠŸç‡: {results['summary']['success_rate']:.2%}")

    if failed_generations:
        print(f"\nâŒ å¤±è´¥çš„é…ç½®:")
        for failure in failed_generations:
            print(f"   - {failure['name']}: {failure['error']}")

    return results


def main():
    parser = argparse.ArgumentParser(description="æ‰¹é‡ Wan2.2 Animate LoRA æ¨ç†")

    # åŸºç¡€å‚æ•°
    parser.add_argument("--base_model_path", type=str, required=True, help="åŸºç¡€æ¨¡å‹è·¯å¾„")
    parser.add_argument("--lora_path", type=str, required=True, help="LoRA æƒé‡è·¯å¾„")

    # é…ç½®å‚æ•°
    parser.add_argument("--config_file", type=str, help="é…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--output_dir", type=str, default="./batch_outputs", help="è¾“å‡ºç›®å½•")

    # ç”Ÿæˆå‚æ•°
    parser.add_argument("--device", type=str, default="cuda", help="è®¾å¤‡")
    parser.add_argument("--dtype", type=str, default="float16", choices=["float16", "float32"], help="æ•°æ®ç±»å‹")
    parser.add_argument("--max_workers", type=int, default=1, help="æœ€å¤§å¹¶è¡Œæ•°")

    # å·¥å…·é€‰é¡¹
    parser.add_argument("--create_sample_config", action="store_true", help="åˆ›å»ºç¤ºä¾‹é…ç½®æ–‡ä»¶")
    parser.add_argument("--sample_config_path", type=str, default="sample_config.json", help="ç¤ºä¾‹é…ç½®è·¯å¾„")

    args = parser.parse_args()

    if args.create_sample_config:
        create_sample_config(args.sample_config_path)
        return

    if not args.config_file:
        print("é”™è¯¯: å¿…é¡»æä¾›é…ç½®æ–‡ä»¶è·¯å¾„ï¼Œæˆ–ä½¿ç”¨ --create_sample_config åˆ›å»ºç¤ºä¾‹é…ç½®")
        return

    # è¿è¡Œæ‰¹é‡æ¨ç†
    results = batch_inference(
        base_model_path=args.base_model_path,
        lora_path=args.lora_path,
        config_file=args.config_file,
        output_dir=args.output_dir,
        max_workers=args.max_workers,
        device=args.device,
        dtype=args.dtype
    )

    print(f"\nğŸ‰ æ‰¹é‡ç”Ÿæˆå®Œæˆ! ç»“æœä¿å­˜åœ¨: {os.path.join(args.output_dir, 'batch_results.json')}")


if __name__ == "__main__":
    # è®¾ç½®æ—¥å¿—
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )

    main()